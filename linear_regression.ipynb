{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROYECTO OPEN DATA II: IMPLEMENTACIÓN DEL ALGORÍTMO DE REGRESIÓN LINEAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la segunda parte de esta asignatura nos hemos sumergido en el mundo de programación con pyspark, utilizado en la dinámica de programación en distribuido. A través de pyspark hemos trabajado con un algoritmo de machine learning, asi pues pudiendo trabajar con las bases de uno de los fenómenos más populares en el mercado ahora mismo.\n",
    "\n",
    "Hemos optado por la implementación de un algoritmo de regresión lineal, cuyo propósito es establecer un modelo para la relación entre características y una variable objetivo. Para el dataset con el que trabajamos nosotras, la primera consiste de una serie de calificaciones optenidas por cierto estudiante y la segunda será la probabilidad de entrada de este estudiante en un máster en concreto.\n",
    "\n",
    "Nuestros parámetros son los siguientes:\n",
    "1. GRE Scores ( de 0 a 340 )\n",
    "2. TOEFL Scores ( de 0 a120 )\n",
    "3. Valoración de la universidad (de 0 a 5 )\n",
    "4. Declaración de propósito y carta de recomendación (de 0 a 5)\n",
    "5. GPA Scores (de 0 a 10)\n",
    "6. Experiencia en investigación (0 o 1)\n",
    "7. Probabilidad de ser admitido (entre 0 y 1)\n",
    "\n",
    "Aplicamos nuestro algoritmo, y lo utilizamos para calcular algunos valores que nos ayudan a medir la eficacia de este. A continuación manejamos una mejora del modelo en cuestión, empleando técnicas de hyper-tuning de los parámetros y grid search, y acabando con una extracción de características y PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparación del entorno de trabajo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el entorno de programcación con pyspark, importandonos los módulos necesarios.\n",
    "En este caso trabajamos con SparkContext, SparkSession y SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[3] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[3]\")\n",
    "print(sc)\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación nos descargamos el dataset y realizamos la limpieza necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Serial No.: int, GRE Score: int, TOEFL Score: int, University Rating: int, SOP: double, LOR : double, CGPA: double, Research: int, Chance of Admit : double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true',inferschema='true').load(\"Admission_Predict.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo hemos considerado necesario cambiar el nombre de una columna, ya que tenía un punto que en algunos métodos daba problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Serial No.\", \"Serial No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|Serial No|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit |\n",
      "+---------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|        1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|\n",
      "|        2|      324|        107|                4|4.0| 4.5|8.87|       1|            0.76|\n",
      "|        3|      316|        104|                3|3.0| 3.5| 8.0|       1|            0.72|\n",
      "|        4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|\n",
      "|        5|      314|        103|                2|2.0| 3.0|8.21|       0|            0.65|\n",
      "+---------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Serial No: integer (nullable = true)\n",
      " |-- GRE Score: integer (nullable = true)\n",
      " |-- TOEFL Score: integer (nullable = true)\n",
      " |-- University Rating: integer (nullable = true)\n",
      " |-- SOP: double (nullable = true)\n",
      " |-- LOR : double (nullable = true)\n",
      " |-- CGPA: double (nullable = true)\n",
      " |-- Research: integer (nullable = true)\n",
      " |-- Chance of Admit : double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alogritmo de Regresión Lineal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, convertimos los datos a dense vector: creamos features y label, las etiquetas con las que suelen trabajar los algoritmos machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,337.0,118.0,...| 0.92|\n",
      "|[2.0,324.0,107.0,...| 0.76|\n",
      "|[3.0,316.0,104.0,...| 0.72|\n",
      "|[4.0,322.0,110.0,...|  0.8|\n",
      "|[5.0,314.0,103.0,...| 0.65|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\").fit(transformed)\n",
    "data = featureIndexer.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|     indexedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|[1.0,337.0,118.0,...| 0.92|[1.0,337.0,118.0,...|\n",
      "|[2.0,324.0,107.0,...| 0.76|[2.0,324.0,107.0,...|\n",
      "|[3.0,316.0,104.0,...| 0.72|[3.0,316.0,104.0,...|\n",
      "|[4.0,322.0,110.0,...|  0.8|[4.0,322.0,110.0,...|\n",
      "|[5.0,314.0,103.0,...| 0.65|[5.0,314.0,103.0,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos en datos de entrenamiento (training) y de testeo (test) (60% para training y 40% para testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2.0,324.0,107.0,...| 0.76|\n",
      "|[3.0,316.0,104.0,...| 0.72|\n",
      "|[5.0,314.0,103.0,...| 0.65|\n",
      "|[6.0,330.0,115.0,...|  0.9|\n",
      "|[7.0,321.0,109.0,...| 0.75|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,337.0,118.0,...| 0.92|\n",
      "|[4.0,322.0,110.0,...|  0.8|\n",
      "|[8.0,308.0,101.0,...| 0.68|\n",
      "|[15.0,311.0,104.0...| 0.61|\n",
      "|[17.0,317.0,107.0...| 0.66|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos algoritmo de Regresion Lineal. Tiene tres parámetros, a los que para empezar ponemos valores por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=2, regParam=0.5, elasticNetParam=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos arquitectura Pipeline (segmentación de instrucciones): Permite implementar el paralelismo a nivel de instrucción en un único procesador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un resumen de nuestro modelo. Sacamos los resultados de los siguientes:\n",
    "* **Error cuadrático medio**: un estimador que mide el promedio de los errores al cuadrado, es decir, la diferencia entre el estimador y lo que se estima\n",
    "* **RMSE**: mide las diferencias entre los valores predichos por un modelo o un estimador y los valores observados.\n",
    "* **R cuadrado**: (coeficiente de determinación) para predecir futuros resultados/probar una hipótesis (como de bien se pueden predecir futuros resultados). Se puede interpretar con como de cerca están los datos a la linea de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = model.stages[-1]\n",
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    Summary=model.summary\n",
    "    \n",
    "    print (\"##\",'-----------------------------------------------------')\n",
    "    print (\"##\",\"Error Cuadrático Medio: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"R cuadrado: %f\" % Summary.r2, \", \\\n",
    "    Total iteraciones: %i\"% Summary.totalIterations)\n",
    "    print (\"##\",'-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsummary(model.stages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a realizar predicciones con el test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación de resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Raiz del error cuadrático medio (RMSE) de los datos de prueba = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred) \n",
    "print('valor r2_: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mejora del modelo: Hyper-tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hyper-tunning trata de encontrar la mejor opción de combinación de valores de los parámetros de entrada de nuestro modelo. De tal forma podemos entrenar nuestro modelo previo para encontrar su rendimiento más óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search** es un algoritmo que itera a través de la lista de valores de los parámetros y estima los modelos de manera independiente y escoje la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, especificamos a nuestro modelo la lista de parámetros sobre la que vamos a iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "import pyspark.ml.classification as cl\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "linear = LinearRegression(labelCol='label',featuresCol = 'indexedFeatures')\n",
    "grid = tune.ParamGridBuilder().addGrid(linear.maxIter, [2, 10, 30]).addGrid(linear.regParam, [0.01, 0.07, 0.5]).addGrid(linear.elasticNetParam, [0.5, 0.4, 0.8]).build()\n",
    "\n",
    "# BinaryClassificationEvaluator lo utilizamos para comparar los modelos (a través de la comparación de su rendimiento)\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=linear, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages=[featureIndexer])\n",
    "model2 = pipeline2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvModel nos devuelve el mejor modelo estimado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(model2.transform(trainingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos el area bajo la curva ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = model2.transform(testData)\n",
    "results = cvModel.transform(data_train)\n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación de los mejores parámetros para nuestro modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue} \n",
    "            for key, paramValue \n",
    "            in zip(\n",
    "                params.keys(), \n",
    "                params.values())\n",
    "        ], metric\n",
    "    ) \n",
    "    for params, metric \n",
    "    in zip(\n",
    "        cvModel.getEstimatorParamMaps(), \n",
    "        cvModel.avgMetrics\n",
    "    )\n",
    "]\n",
    "\n",
    "sorted(results, key=lambda el: el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (MaxIter): ', cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (RegParam): ', cvModel.bestModel._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (ElasticNetParam): ', cvModel.bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los mejores parámetros para nuestro modelo, lo entrenamos con estos parámetros para obtener la mejor predicción en base a nuestros datos de entrada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
